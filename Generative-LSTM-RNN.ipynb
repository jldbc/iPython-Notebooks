{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#Generative LSTM RNN\n",
    "This is a recurrent neural network with long short term memory (LSTM) cells, used for procedural text generation. The network trains on sequences of characters from a given text. Given a seed sequence, the trained model can then iteratively guess the next character in the sequence. The result is the generation of an original text output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommendations**\n",
    "\n",
    "This will be quite slow to run on a CPU. If doing so, I recommend using a small text file. Even better, if you have access to a GPU, use that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acknowledgements**\n",
    "\n",
    "This is based on a term project I worked on with several partners. Contributors for the RNN code include myself, Drew Hoo, and Emily Lu. This was also largely inspired by work by Francois Chollet, who provides an LSTM example in his Keras library (keras/examples/lstm_text_generation.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import traceback\n",
    "import pdb\n",
    "from os import path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Hyperparameters\n",
    "These are some basic features that can be tweaked to impact model performance. I'm hoping to eventually remove some of the abstraction that Keras provides in order to allow greater control of the model via more tunable parameters in this section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path = \"/Users/jamesledoux/Documents/data_exploration/author_files/shakespeare\"\n",
    "num_epochs = 100\n",
    "sequence_length = 80\n",
    "step = 3 #size of steps to use between sequence starting points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Preparing the Data\n",
    "Recurrent networks take in data as sequences. There are a few necessary steps in order to turn raw text data in to tensors that we can train a network on. In these next few cells we:\n",
    "* Read in the data and extract some basic information from it\n",
    "* Build translation dictionaries to convert between machine-readable numerical character IDs and human-readable  characters\n",
    "* Create sequences of text and convert them to numerical IDs\n",
    "* Populate a one-hot encoded tensor of dimensions (num_sequences, sequence_length, num_characters) to use as input data, with a corresponding y-matrix of dimensions (num_sequences, num_characters) as the target to train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text info: len 163631, type <type 'str'>\n",
      "Total Unique Chars:  47\n"
     ]
    }
   ],
   "source": [
    "text = open(file_path).read().lower()\n",
    "len_text = len(text)\n",
    "print 'Text info: len {}, type {}'.format(len_text, type(text))\n",
    "\n",
    "# Get Unique chars from text\n",
    "chars = sorted(list(set(text)))\n",
    "len_chars = len(chars)\n",
    "print 'Total Unique Chars: ', len_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up translation dicts\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number sequences:  54517\n",
      "Vectorizing...\n",
      "Space X: 204984048 Bytes\n",
      "Space_y: 2562411 Bytes\n"
     ]
    }
   ],
   "source": [
    "maxlen = sequence_length\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "# step through text file creating sequences\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    end_index = i + maxlen\n",
    "    sentences.append(text[i: end_index])\n",
    "    next_chars.append(text[end_index])\n",
    "print 'Total number sequences: ', len(sentences)\n",
    "\n",
    "# Start making your sparse matrices\n",
    "print 'Vectorizing...'\n",
    "X = np.zeros((len(sentences), maxlen, len_chars), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len_chars), dtype=np.bool)\n",
    "\n",
    "# Check space complexity\n",
    "space_X = sys.getsizeof(X)\n",
    "space_y = sys.getsizeof(y)\n",
    "print 'Space X: {} Bytes'.format(space_X)\n",
    "print 'Space_y: {} Bytes'.format(space_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build the final matrix of sequences to be used in training\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Building the Model\n",
    "Keras is a high-level library that makes building TensorFlow and Theano models easy. Here we create a model with two LSTM layers with 512 nodes each, with a fully-connected, softmax-activated layer at the end to classify the observations by which character should come next. We use Dropout between LSTM layers for regularization.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(X_shape_1, X_shape_2, y_shape, prev_epoch_counter):\n",
    "    # define the LSTM model via our old code - No callbacks for now\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, input_shape=(X_shape_1, X_shape_2), return_sequences=True))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(LSTM(512))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(y_shape, activation='softmax'))\n",
    "    \n",
    "    # Check if weights file exists, should not exist only for first run\n",
    "    file_path = 'char_training/model_weights_' + str(prev_epoch_counter) + '.h5'\n",
    "    print 'File path of model weights: ', file_path\n",
    "    if path.isfile(file_path):\n",
    "        print 'found file, loading weights... '\n",
    "        model.load_weights(file_path)\n",
    "    else:\n",
    "        print '.h5 file not found'\n",
    "    # Compile and return model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Saving Model Weights\n",
    "You can save weights in Keras in order to keep a model that you're happy with. These models are slow to train, so weight saving is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_history(history):\n",
    "    print \"Saving History\"\n",
    "    with open('character-training_history.json', 'w') as f:\n",
    "        json.dump(history.history, f)\n",
    "    print 'History Saved'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling Function for Output Generation\n",
    "Credit to Francois Chollet on this. By varying the temperature parameter here, you can either increase or decrease the diversity of the model's predictions. We experiment with varying levels of this parameter during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_shape_1, X_shape_2 = X.shape[1], X.shape[2]\n",
    "y_shape = y.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Generating Output\n",
    "We write this function before the model trains so that we can view sequences of generated output at the end of each epoch. This takes a sentence as a seed for the model, and iteratively generates what the model thinks should come next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_output(model, len_text, maxlen, len_chars, char_indices, indices_char, text, epoch):\n",
    "    stdout = sys.stdout\n",
    "    output_path = 'char_lstm_output_files/lstm_output_text_{:02d}.txt'.format(epoch)\n",
    "    sys.stdout = open(output_path, 'w')\n",
    "    start_index = random.randint(0, len_text - maxlen - 1)\n",
    "\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print '\\n Output for Epoch {:02d} with diversity {}'.format(epoch, diversity)\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print 'Seed: \"' + sentence + '\" \\n'\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(1000):\n",
    "            x = np.zeros((1, maxlen, len_chars))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "    sys.stdout = stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Training the Model\n",
    "For a set number of epochs, this script trains the model and saves its weights. A simple improvement to this would be implementing early stopping, to avoid waiting on the model to train once it begins to overfit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Starting Epoch 0 ... \n",
      "File path of model weights: "
     ]
    }
   ],
   "source": [
    "full_start_time = datetime.now()\n",
    "for epoch in range(num_epochs):\n",
    "    print '\\n Starting Epoch {} ... '.format(epoch)\n",
    "    model = build_model(X_shape_1, X_shape_2, y_shape, epoch - 1)\n",
    "    start_time = datetime.now()\n",
    "    # Commented out callbacks for now\n",
    "    history = model.fit(X, y, validation_split=0.20, nb_epoch=1, batch_size=512, verbose=1)\n",
    "    model_total_time = datetime.now() - start_time\n",
    "    print \"training time: \" + str(model_total_time)\n",
    "    save_history(history)\n",
    "\n",
    "    # Save the weights from the training\n",
    "    print '\\n Saving weights ...'\n",
    "    model_weights = 'char_training/model_weights_' + str(epoch) + '.h5'\n",
    "    model.save_weights(model_weights)\n",
    "    print 'Weights Saved'\n",
    "    gen_output(model, len_text, maxlen, len_chars, char_indices, indices_char, text, epoch)\n",
    "    print '\\n Finished output of Epoch: {}'.format(epoch)\n",
    "\n",
    "total_time = datetime.now() - full_start_time\n",
    "print \"Semi-total Run Time: \" + str(model_total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
