{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textfile = \"data/potter\"\n",
    "textfile = open(textfile).read().lower()\n",
    "badtext = '/:;][{}-=+)(*&^%$#@!~1234567890'\n",
    "textfile = textfile.translate(None, badtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def translation_dicts(textfile):\n",
    "    chars = sorted(list(set(textfile)))\n",
    "    char_to_num_dict = {}\n",
    "    num_to_char_dict = {}\n",
    "    num = 0\n",
    "    for c in chars:\n",
    "        char_to_num_dict[c] = num\n",
    "        num_to_char_dict[num] = c\n",
    "        num += 1\n",
    "    return num_to_char_dict, char_to_num_dict, chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_to_char_dict, char_to_num_dict, chars = translation_dicts(textfile)\n",
    "\n",
    "sequence_length = 50 #num of chars per observation\n",
    "beg_index = 0\n",
    "end_index = sequence_length\n",
    "X_mat = []\n",
    "y_vector = []\n",
    "\n",
    "for i in range(len(textfile) - sequence_length):\n",
    "    x_row = textfile[i:i+sequence_length]\n",
    "    x_row = [char_to_num_dict[j] for j in x_row]\n",
    "    y_val = textfile[i+sequence_length]  #the character you're predicting based on the previous 100\n",
    "    y_val = char_to_num_dict[y_val]\n",
    "    X_mat.append(x_row)\n",
    "    y_vector.append(y_val)\n",
    "\n",
    "y_vector = np.asarray(y_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert y to one-hot encoding\n",
    "y_mat = np_utils.to_categorical(y_vector)\n",
    "\n",
    "num_patterns = len(X_mat)\n",
    "num_vocab = len(chars)\n",
    "X = np.reshape(X_mat, (num_patterns, sequence_length, 1))\n",
    "X = X/float(num_vocab) #normalize X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))  #WE NEED TO GO DEEPER!!!!\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_mat.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without further ado . . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "20992/21032 [============================>.] - ETA: 0s - loss: 3.0797Epoch 00000: loss improved from inf to 3.07981, saving model to weights-improvement-00-3.0798.hdf5\n",
      "21032/21032 [==============================] - 238s - loss: 3.0798   \n",
      "Epoch 2/5\n",
      "20992/21032 [============================>.] - ETA: 0s - loss: 3.0534Epoch 00001: loss improved from 3.07981 to 3.05345, saving model to weights-improvement-01-3.0534.hdf5\n",
      "21032/21032 [==============================] - 242s - loss: 3.0534   \n",
      "Epoch 3/5\n",
      "20992/21032 [============================>.] - ETA: 0s - loss: 2.9931Epoch 00002: loss improved from 3.05345 to 2.99269, saving model to weights-improvement-02-2.9927.hdf5\n",
      "21032/21032 [==============================] - 247s - loss: 2.9927   \n",
      "Epoch 4/5\n",
      "20992/21032 [============================>.] - ETA: 0s - loss: 2.8595Epoch 00003: loss improved from 2.99269 to 2.85926, saving model to weights-improvement-03-2.8593.hdf5\n",
      "21032/21032 [==============================] - 227s - loss: 2.8593   \n",
      "Epoch 5/5\n",
      "20992/21032 [============================>.] - ETA: 0s - loss: 2.7763Epoch 00004: loss improved from 2.85926 to 2.77620, saving model to weights-improvement-04-2.7762.hdf5\n",
      "21032/21032 [==============================] - 227s - loss: 2.7762   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10c13cf90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the model (only doing this for a few epochs as an example. going to load a pre-trained model to save time here.)\n",
    "model.fit(X, y_mat, batch_size=64, nb_epoch=5, callbacks=callbacks_list)  #increase num epoch once we get this working well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" ow my whistle, you kick off from the ground, hard,\" said\n",
      "madam hooch. \"keep your brooms steady, rise a few feet, and then come\n",
      "straight back down by leaning forward slightly. on my whistle  three\n",
      " two \"\n",
      "but neville, nervous and jumpy and frightened of being left on the\n",
      "ground, pushed off hard before the whistle had touched madam hooch's\n",
      "lips.\n",
      "\"come back, boy\" she shouted, but neville was rising straight up like a\n",
      "cork shot out of a bottle  twelve feet  twenty feet. harry saw his\n",
      "scared white face look down at the ground falling away, saw him gasp,\n",
      "slip sideways off the broom and \n",
      "wham  a thud  \"\n",
      "coagon baterts, notter. thich tas a had to talking alother and thene he dound hear thought he sas standing uotil the tall. harry and hermione had fooward at the thire pieces and talked to to the last had mulled out of the coor and foog to to the gash tas a hood craging ar the tall. harry had reen in"
     ]
    }
   ],
   "source": [
    "#load dicts from the original data this was trained on\n",
    "char_to_num_dict = {'\\t': 0, '\\n': 1, ' ': 2, '\"': 3, \"'\": 4, ',': 5, '.': 6, '?': 7, '\\\\': 8, 'a': 9, 'c': 11, 'b': 10, 'e': 13, 'd': 12, 'g': 15, 'f': 14, 'i': 17, 'h': 16, 'k': 19, 'j': 18, 'm': 21, 'l': 20, 'o': 23, 'n': 22, 'q': 25, 'p': 24, 's': 27, 'r': 26, 'u': 29, 't': 28, 'w': 31, 'v': 30, 'y': 33, 'x': 32, 'z': 34}\n",
    "num_to_char_dict = {0: '\\t', 1: '\\n', 2: ' ', 3: '\"', 4: \"'\", 5: ',', 6: '.', 7: '?', 8: '\\\\', 9: 'a', 10: 'b', 11: 'c', 12: 'd', 13: 'e', 14: 'f', 15: 'g', 16: 'h', 17: 'i', 18: 'j', 19: 'k', 20: 'l', 21: 'm', 22: 'n', 23: 'o', 24: 'p', 25: 'q', 26: 'r', 27: 's', 28: 't', 29: 'u', 30: 'v', 31: 'w', 32: 'x', 33: 'y', 34: 'z'}\n",
    "    \n",
    "n_chars = len(textfile)\n",
    "n_vocab = len(chars)\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 600\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = textfile[i:i + seq_length]\n",
    "    seq_out = textfile[i + seq_length]\n",
    "    dataX.append([char_to_num_dict[char] for char in seq_in])\n",
    "    dataY.append(char_to_num_dict[seq_out])\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "#print \"Total Patterns: \", n_patterns\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_mat.shape[1], activation='softmax'))\n",
    "\n",
    "# load the network weights\n",
    "#filename = \"/Users/jamesledoux/Desktop/weights-improvement-13-1.9537.hdf5\"  #dante\n",
    "#filename = \"/Users/jamesledoux/Desktop/weights-improvement-31-1.4242.hdf5\"  #willie\n",
    "#filename = \"/Users/jamesledoux/Desktop/shakespeare-1.3267.hdf5\"  #willie\n",
    "filename = weightfile = \"data/rowling-1.2660.hdf5\"\n",
    "#filename = \"/Users/jamesledoux/Desktop/weights-improvement-46-1.4188.hdf5\"  #dante\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX) - 1)\n",
    "pattern = dataX[start]\n",
    "# a random seed taken from the text\n",
    "print \"Seed:\"\n",
    "print \"\\\"\", ''.join([num_to_char_dict[value] for value in pattern]), \"\\\"\"\n",
    "\n",
    "# trying out a custom seed\n",
    "#pattern = \"                                                            to be or not to be that is the question,\"\n",
    "#print pattern\n",
    "#pattern = [char_to_int[c] for c in pattern]\n",
    "\n",
    "#generate characters\n",
    "for i in range(300):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = num_to_char_dict[index]\n",
    "    seq_in = [num_to_char_dict[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
