{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GroupMe Analytics\n",
    "## Data Mine your Group Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prerequisites: \n",
    "* install groupy (pip install groupy)\n",
    "* Be sure you're using Python3\n",
    "* find your access token on [dev.groupme.com](https://dev.groupme.com/)  \n",
    "* save it in your home directory as .groupy.key (echo 'your_access_token' > ~/.groupy.key )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import groupy\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys, re\n",
    "import numpy as np\n",
    "from random import random\n",
    "from operator import add\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#note: need groupy api key saved in user's home directory for this to work\n",
    "groups = groupy.Group.list()\n",
    "print(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#replace the index below with whichever group you want to analyze from the above printed list\n",
    "group = groups[1]\n",
    "members = group.members()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: feeding messages into a data frame + extracting features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bonus points: all profanity printed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#populate messages list \n",
    "temp_msgs = []\n",
    "messages = group.messages()\n",
    "profanity_list = ['fuck', 'shit', 'fuk', 'damn', 'bitch', 'ass', ' ass ', ' ass.', 'douche', 'asshole', 'douchebag', 'fucker', 'dick', 'cunt', 'fucking']\n",
    "profanity_list_2 = ['fuck', 'shit', 'fuk', 'damn', 'bitch', ' ass ', ' ass.', 'douche', 'asshole', 'douchebag', 'fucker', 'dick', 'cunt', 'fucking']\n",
    "\n",
    "for i in range(1000):  #1000 is an arbitrary number. This can be decreased unless your group chat is huge.\n",
    "    try:\n",
    "        if len(messages) != 0:\n",
    "            temp_msgs.append(messages)\n",
    "            messages = messages.older()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "msgs = []\n",
    "for i in temp_msgs:\n",
    "    for j in i:\n",
    "        msgs.append(j)\n",
    "msgs_2 = msgs\n",
    "    \n",
    "#put features into tuples \n",
    "for i in range(len(msgs)):\n",
    "    ex = msgs[i]\n",
    "    #print(ex.likes())\n",
    "    self_like = 0\n",
    "    fucks_given = 0\n",
    "    profanity = 0\n",
    "    selfname = ex.name\n",
    "    for name in ex.likes():\n",
    "        if str(name) == str(selfname):\n",
    "            self_like = 1\n",
    "    try:\n",
    "        text = str(ex.text).lower()  #lowercase\n",
    "    except:\n",
    "        text = str(ex.text)\n",
    "    if any(word in text for word in profanity_list_2):\n",
    "        print(text)\n",
    "    text2 = text.split()\n",
    "    for word in text2:\n",
    "        if(word in profanity_list):\n",
    "            profanity = profanity + 1\n",
    "    if 'fuck' in text:\n",
    "        fucks_given = 1\n",
    "    tup = (ex.name, ex.user_id, ex.created_at, text,ex.favorited_by, ex.likes(), len(ex.likes()), self_like, profanity, fucks_given, len(text2))\n",
    "    msgs[i] = tup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame from above processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colnames = ['name', 'uid', 'datetime', 'text', 'favs', 'likes', 'num_likes', 'self_like', 'profanity', 'fucks_given', 'wordcount']\n",
    "df = pd.DataFrame.from_records(msgs, columns = colnames)\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most liked messages in the group chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"most liked messages in groupme history\")\n",
    "df.sort_values(by=\"num_likes\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most profane messages in the group chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"most profane messages\")\n",
    "df.sort_values(by=\"profanity\", ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Users DataFrame for aggregate info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users = pd.DataFrame(np.asarray(members), columns = [\"name\"])\n",
    "#most active members\n",
    "\n",
    "users['num_msgs'] = 0\n",
    "users['mean_message_length'] = 0\n",
    "users['num_likes'] = 0\n",
    "users['self_likes'] = 0\n",
    "users['profanity'] = 0\n",
    "users['fucks_given'] = 0\n",
    "\n",
    "\n",
    "for i in members:\n",
    "    num_msgs = df[df['name']==str(i)].shape[0]\n",
    "    users.loc[users['name'] == i, 'num_msgs'] = num_msgs\n",
    "    wordcounts = df[df['name']==str(i)]['wordcount']\n",
    "    wordcounts = np.mean(wordcounts)\n",
    "    users.loc[users['name'] == i, 'mean_message_length'] = wordcounts\n",
    "    num_likes = df[df['name']==str(i)]['num_likes'].sum()\n",
    "    users.loc[users['name'] == i, 'num_likes'] = num_likes\n",
    "    self_likes = df[df['name']==str(i)]['self_like'].sum()\n",
    "    users.loc[users['name'] == i, 'self_likes'] = self_likes\n",
    "    profanity = df[df['name']==str(i)]['profanity'].sum()\n",
    "    users.loc[users['name'] == i, 'profanity'] = profanity\n",
    "    fucks_given = df[df['name']==str(i)]['fucks_given'].sum()\n",
    "    users.loc[users['name'] == i, 'fucks_given'] = fucks_given  \n",
    "    \n",
    "users['likes_per_msg'] = users['num_likes']/users['num_msgs']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize features in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Bar chart of message counts\n",
    "names = np.asarray(users['name'])\n",
    "counts = np.asarray(users['num_msgs'])\n",
    "for i in range(len(names)):\n",
    "    names[i] = str(names[i])\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.xticks(np.linspace(1,len(names), num=len(names)), names, rotation=30)\n",
    "plt.bar(np.linspace(1,len(names), num=len(names)), counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Lexicon: Which words best define each member of the group chat? Which are the most commonly used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TFIDF Scores \n",
    "#first, collapse everyone's messages into personalized documents \n",
    "documents = []\n",
    "for i in members:\n",
    "    document = \"\"\n",
    "    msgs = df[df['name']==str(i)]['text']\n",
    "    for i in msgs:\n",
    "        document += (str(i) + \" \")\n",
    "    documents.append(document)\n",
    "\n",
    "\n",
    "#create the vectorizer\n",
    "tfidf = TfidfVectorizer(max_df=0.9,\n",
    "                        ngram_range=(1, 1),\n",
    "                        stop_words='english',\n",
    "                        strip_accents='unicode', analyzer = 'word')\n",
    "\n",
    "#run the multiplicaiton\n",
    "tfidf_matrix =  tfidf.fit_transform(documents)\n",
    "feature_names = tfidf.get_feature_names() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most common words in the group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indices = np.argsort(tfidf.idf_)#[::1]\n",
    "tfidf_matrix_2 = tfidf_matrix.todense()\n",
    "top_n = 100\n",
    "top_features = [feature_names[i] for i in indices[:top_n]]\n",
    "print(\"Most used words in the groupme: \")\n",
    "print(top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "dicts = []\n",
    "for person in members:\n",
    "    persondict = {}\n",
    "    for word in feature_names:\n",
    "        if tfidf_matrix[i,j] != 0:\n",
    "            persondict[word] = tfidf_matrix[i,j]\n",
    "        j += 1\n",
    "    j = 0\n",
    "    i += 1\n",
    "    #print('person done')\n",
    "    dicts.append(persondict)\n",
    "#sorted(tfidf_matrix_2[5].tolist()[0])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "j = 0\n",
    "words_dict = {}\n",
    "for i in dicts:\n",
    "    sorted_vals = sorted(i.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    words_dict[str(members[j])] = sorted_vals[0:15]\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most distinctive words per person \n",
    "\n",
    "* Determined by TF-IDF scores\n",
    "* Essentially, the words that each person uses way more often than everyone else "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for entry in words_dict:\n",
    "    print(\"Top words for \" + str(entry) + \": \")\n",
    "    for w in words_dict[entry]:\n",
    "        print(w)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some time series stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Usage patterns\n",
    "print(\"Groupme useage by month\")\n",
    "df.groupby(df.datetime.dt.month).count().plot(kind=\"bar\", legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Usage patterns\n",
    "print(\"Groupme useage by hour of day\")\n",
    "df.groupby(df.datetime.dt.hour).count().plot(kind=\"bar\", legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### running use frequency by day/hour since the group chat was created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#note: this one takes a sec to complete if there is a large amount of data\n",
    "print('Usage since inception')\n",
    "n = 20\n",
    "\n",
    "ax = df.groupby(df.datetime.dt.date).count().plot(kind=\"bar\", legend=False, rot=30, fontsize=8, figsize=(14,6))\n",
    "ticks = ax.xaxis.get_ticklocs()\n",
    "ticklabels = [l.get_text() for l in ax.xaxis.get_ticklabels()]\n",
    "ax.xaxis.set_ticks(ticks[::n])\n",
    "ax.xaxis.set_ticklabels(ticklabels[::n])\n",
    "\n",
    "ax.figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
